{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Neural ODEs for Lattice Field Theory**\n",
        "---\n",
        "\n",
        "Let us stare at this screenshot from Prof. Miranda Cheng's talk.\n",
        "![picture](https://drive.google.com/uc?export=view&id=1CrbOMDeMZVTHcdFtGjeza6dVmEWZOGCg)\n",
        "Suppose, we want to train a Neural Network perfectly, such that inputs are $x, y \\in V_L$ on lattice, and outputs are $\\phi_x \\sim e^{- S[\\phi]}$, sampled from a target lattice field theory, with action\n",
        "$$S[\\phi] = \\sum\\limits_{x,y \\in V_L} \\phi_x \\Delta_{x,y} \\phi_y + \\sum\\limits_{x \\in V_L} (m^2 \\phi_x^2 + \\lambda \\phi_x^4).$$\n",
        "\n",
        "Such a perfectly trained Neural Network can be recast as an ODE\n",
        "$$\\frac{d}{dt}\\phi(t)_x = v_{\\theta}(\\phi(t), t, \\lambda)_x,$$\n",
        "where ODE time $t \\in \\{0, \\cdots , T \\}$ corresponds to hidden layers. This ODE transforms Neural Network input $z:= z(0)$ to the desired output $\\phi_x \\sim e^{-S[\\phi]}$. A continuous function $v_{\\theta}$ represents the trained Network function, when the number of hidden layers is large, and training step size is infinitesimally small.\n",
        "\n",
        "Such a Neural Network is called a *Neural ODE*."
      ],
      "metadata": {
        "id": "N34ruOtNeQ1m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From generic Neural Networks to Neural ODEs**\n",
        "----\n",
        "\n",
        "> [Reference paper](https://arxiv.org/pdf/1806.07366.pdf)\n",
        ">\n",
        ">A Neural ODE acts as a vector field between initial state $z(0)$ and target state $z(T)$.  \n",
        ">\n",
        "<!-- ![picture](https://drive.google.com/uc?export=view&id=1gI0Thz0PiGYfaH0M6a0aeO0zKAkZCu3v) -->\n",
        ">\n",
        "\n",
        "The following ODE\n",
        "$$\\frac{dz}{dt} = v_{\\theta}(z,t),~~ z(0) = z,~~z(T)=\\phi_x.$$\n",
        "can be solved using Euler's method, where the ODE time is discretized into $n$ time-steps. The solution to this ODE, at any time step $t_{n+1}$, is\n",
        "$$z_{n+1} = z_n + v_{\\theta}(z_n, t_n) \\cdot(t_{n+1} - t_n), $$\n",
        "for $z_i := z(t_i).$"
      ],
      "metadata": {
        "id": "USQGMXLBCxT2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us introduce an ODE solver function called ```odeint``` from ```jax.experimental.ode``` Library.\n",
        "----\n",
        "\n",
        "This function\n",
        "$$\\text{odeint}(v_{\\theta}, z_n, \\text{array}(t_0, t_T), \\theta_{t_n})$$\n",
        "\n",
        "results in a tuple of the form (initial state $z_0$, target state $z_T$). In the context of Neural ODEs, $\\theta_{t_n}$ are the network parameters of the $n^{\\text{th}}$ hidden layer, and get updated via training.  \n",
        "\n",
        "\n",
        ">Then, we need to define a loss function $L()$ that can be used to train a randomly initialized Network to produce a Neural ODE corresponding to a given problem. Consider the following loss function\n",
        "$$L(\\,z(t_{n+1})\\,) = L \\Big( \\, z(t_n) + \\int^{t_{n+1}}_{t_n} v_{\\theta_{t_n}}(z_{t_n} , t_n , \\theta_{t_n}) \\, \\Big) . $$\n",
        "> The input to this loss function is an ODE solver of the form $\\text{ODESolve}(z_{t_n}, v_{\\theta_{t_n}}, t_n , t_{n+1})$."
      ],
      "metadata": {
        "id": "T2R8OdNZDbu_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1PoFEfjcpHm"
      },
      "source": [
        "**Example 0**\n",
        "---\n",
        "\n",
        "Consider initial state $z(0)=x$, and target state $z(T)=x^3 + 0.1x$. We want to construct a Neural ODE, starting from a simple fully connected feedforward Neural Network, using the loss function defined above. The trained Neural Network / the Neural ODE would correspond to a vector field $f: z(0) \\to z(T)$."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Given below are the steps to get this Neural ODE. Some of these steps are left as exercises.**\n",
        "---"
      ],
      "metadata": {
        "id": "uJSygMjWQVg5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mL9c10Gd7JG"
      },
      "source": [
        "* First, we import necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1t3vrVctMetz"
      },
      "outputs": [],
      "source": [
        "import jax.numpy as jnp\n",
        "import numpy.random as nprand\n",
        "from jax import grad, jit, vmap\n",
        "from jax import random\n",
        "from jax.experimental.ode import odeint\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffoIG0a5jK1X"
      },
      "source": [
        "* Let us define the initial and final (target) states, $$z(0)=0, \\\\ z(T)=x^3 + 0.1x,$$ respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfmBQ7REjOb_"
      },
      "outputs": [],
      "source": [
        "input = jnp.reshape(jnp.linspace(-2.5, 2.5, 15), (15, 1))\n",
        "target = input**3 + 0.1 * input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4qdq4lllofa"
      },
      "source": [
        "* We want to draw initial Network parameters $\\theta$ randomly, and update these through training. Given below is a function to draw parameters randomly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CevNj4ptl06R"
      },
      "outputs": [],
      "source": [
        "def init_random_params( N, rng=nprand.RandomState(0)): # N denotes the size or width of each hidden layer of the Neural ODE that we want\n",
        "    return [(rng.randn(p, q), rng.randn(q))\n",
        "            for p, q, in zip(N[:-1], N[1:])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnVwYAzpcMry"
      },
      "source": [
        "* Next, we want to define $f: z(0) \\to z(T)$ as a fully connected feedforward Network. To do that, we define a function ```fcn``` that creates a fully connected feedforward Network from input ```xs``` and parameters $\\theta$."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fcn(theta, xs):\n",
        "    for w, b in theta:\n",
        "      outputs = jnp.dot(xs, w) + b  # Linear transformation\n",
        "      xs = jnp.tanh(outputs)        # Nonlinearity; we can choose ReLU or other nonlinearities as well\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "oja0NlXdMJ3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 0: Define the function $v_{\\theta_{t_n}}(z_{t_n}, t_n, \\theta_{t_n})$.**\n",
        "---\n"
      ],
      "metadata": {
        "id": "01oAoYlrVl6o"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwBnxisddJsg"
      },
      "source": [
        "**Exercise 1: Define the function $z_{t_{n+1}} = \\text{ODESolve}(z_{t_n}, v_{\\theta_{t_n}}, t_n , t_{n+1})$.**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxIQyeSAeGuz"
      },
      "source": [
        "**Exercise 2: Use function ODESolve to define a quadratic loss function. Call it ```NODE_L2loss```.**\n",
        "---\n",
        "\n",
        "Use the following tips.\n",
        "* Function ```vmap``` can be used to apply function ODESolve to its inputs in a vectorized manner. E.g. ```batched_ODESolve = vmap(ODESolve, in_axes=(None, 0))```\n",
        "* Function ```batched_ODESolve``` takes in two arguments: network parameters, and network input. Network output is ```batched_ODESolve(theta, xs)```.\n",
        "* We define the quadratic loss as $L_2 = (z_{t_{n+1}} - z_T)^2$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMMTUI2Le8EO"
      },
      "source": [
        "**Next,**\n",
        "---\n",
        "\n",
        "* We define a simple gradient descent optimizer, using this loss. Let's call this loss function as ```NODE_L2loss```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_Wup8cjfA5l"
      },
      "outputs": [],
      "source": [
        "def NODE_gd(theta, xs, ys):\n",
        "    gradients = grad(NODE_L2loss)(theta, xs, ys)\n",
        "    return [(w - step_size * dw, b - step_size * db)\n",
        "            for (w, b), (dw, db) in zip(theta, gradients)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSzcNCNAgbx6"
      },
      "source": [
        "* Next, using function ```init_random_params```, we initialize parameters $\\theta$ for a fully connected feedforward Network (one that we intend to optimize enough to create a Neural ODE)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* This requires defining Network hyperparameters."
      ],
      "metadata": {
        "id": "j2pU_3xNEIUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NODE_width = [2, 20, 1] # layer sizes\n",
        "step_size = 0.0005 # size of training steps\n",
        "train_iters = 1000 # number of training steps\n",
        "\n",
        "NODE_theta = init_random_params(NODE_width)"
      ],
      "metadata": {
        "id": "Ul09ykm3ELw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Using these, we initialize the Network, and update it until Network input and output are $z(0)=x$ and $z(T) = x^3 + 0.1x$, respectively.\n",
        "\n"
      ],
      "metadata": {
        "id": "4ALhFdnUF6pL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Note that $T=2$; and layer sizes at $t=0,1,2$ are $N=2,20,1$, respectively."
      ],
      "metadata": {
        "id": "vv7AWwsyMPa8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(train_iters):\n",
        "    NODE_theta = NODE_gd(NODE_theta, input, target)"
      ],
      "metadata": {
        "id": "KM8l2ik-K9UM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* This Neural Network is now a Neural ODE, representing a continuous transformation from inputs $z(0)$ to outputs $z(T)$.  "
      ],
      "metadata": {
        "id": "wRe-qe7UK-fL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Let us test the efficiency of this Neural ODE for a different choice of inputs."
      ],
      "metadata": {
        "id": "qAWTnLm8MkBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(6, 4), dpi=150)\n",
        "ax = fig.gca()\n",
        "ax.scatter(input, target, lw=0.5, color='red')\n",
        "new_inputs = jnp.reshape(jnp.linspace(-3.0, 3.0, 300), (300, 1))\n",
        "ax.plot(new_inputs, batched_ODESolve(NODE_theta, new_inputs), lw=0.5, color='green')\n",
        "ax.set_xlabel('input')\n",
        "ax.set_ylabel('output')\n",
        "plt.legend(('Neural ODE'))"
      ],
      "metadata": {
        "id": "VwgbjrMwLdmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lj7u5qeRl9Jv"
      },
      "source": [
        "**Example 1: Simple Harmonic Oscillator**\n",
        "----\n",
        "\n",
        "Consider the differential equation\n",
        "$$m \\, y''(t) + k\\, y(t) =0,$$\n",
        "for $m= 1.4$, $k=1.9$ (in appropriate units), and boundary conditions are\n",
        "$$y(0)=1.2~,~ y(0.47)=0.09~,~y'(1)=0.$$\n",
        "\n",
        "> The analytical solution (via Mathematica) is $$y(t) = 0.043576 \\cos(1.16496 \\, t) + 0.101414 \\sin(1.16496 \\, t)   .$$\n",
        "\n",
        "We want to train a Neural Network such that its input and output after training are $t$ and $y(t)$, respectively. Such a trained Network would be a Neural ODE for this one-dimensional simple harmonic oscillator.\n",
        "\n",
        "> To do so, we first note that, *any arbitrary ODE of order $n$* for some scalar function $y(t)$\n",
        "$$G\\big(y(t), y'(t), y''(t), \\cdots , y^{(n)}(t),t \\big) = 0,$$\n",
        "can be mapped into **a system of first order ODEs**\n",
        "$$y'_1(t) = y_2(t),\\\\\n",
        "y'_2(t) = y_3(t), \\\\\n",
        "\\vdots \\\\\n",
        "y'_n(t) = G^*\\big(y(t), y'(t), y''(t), \\cdots , y^{(n)}(t),t \\big).\n",
        "$$\n",
        "Here, $G^*$ is obtained by solving $G$ for $y^{(n)}(t)$.\n",
        "\n",
        "Our simple harmonic oscillator problem is a second order ODE. That can be recast as two joint first order ODEs. The trained Network, i.e. the Neural ODE, will produce a tuple made from $\\big(y'(t), y(t) \\big)$ as its output.  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Our goal here is to train a Neural Network to get this Neural ODE. This is broken into a few steps. We will walk you through some of these steps, and leave some as exercises.**\n",
        "----"
      ],
      "metadata": {
        "id": "Q_fOj-MzQiFy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Byu6LhAhsIKb"
      },
      "source": [
        "* First, we import necessary libraries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfAdUfQwrueb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TjSPJgPsLUw"
      },
      "source": [
        "**Exercise 3: Define two functions $y(t)$ and $y'(t)$ for final (target) states of the Neural ODE.**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Next,**\n",
        "---\n",
        "* We generate Neural ODE initial state $t$ and final state $(y(t), y'(t))$ using functions ```y(t)``` and ```y'(t)```, defined above."
      ],
      "metadata": {
        "id": "8AGwNU8ERewk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZebrJBNfrwaC"
      },
      "outputs": [],
      "source": [
        "input = np.random.rand(200)*30\n",
        "target = np.array([y(input), y_prime(input)]).T # y and y_prime here are the functions for y(t) and y'(t), respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9SXBxLVtJfY"
      },
      "source": [
        "* Then, we design a simple Neural Network with tanh activation function, each layer size 100, and use MSE loss and Adam optimizer to compile it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5lBVS8HtZoo"
      },
      "outputs": [],
      "source": [
        "# define the model\n",
        "ODE_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(100, activation=tf.nn.tanh, input_shape=(1,)),\n",
        "    tf.keras.layers.Dense(100, activation=tf.nn.tanh),\n",
        "    tf.keras.layers.Dense(2)\n",
        "])\n",
        "# compile it\n",
        "ODE_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(0.01),\n",
        "    loss=tf.keras.losses.MeanSquaredError()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3o2YUgZcuSLF"
      },
      "source": [
        "* Now that we have initialized this network, we want to train this to get Neural ODE for the simple harmonic oscillator problem."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* First, learning is noisy, and we want to save the best parameters during training to use for future predictions. To do so, we define ```checkpoint``` below."
      ],
      "metadata": {
        "id": "bBFvinxPTL3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\"weights\",\n",
        "    save_weights_only=True, monitor=\"loss\", verbose=1, save_best_only=True)"
      ],
      "metadata": {
        "id": "slFuSEtOTVcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Then train this model on input $t$ and target data $y(t), y'(t)$. We save the training history below.  "
      ],
      "metadata": {
        "id": "wryjR4RCTg1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = ODE_model.fit(input, target, epochs=4500, batch_size=100,\n",
        "    callbacks=[checkpoint])"
      ],
      "metadata": {
        "id": "pV4Cr5BETsIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 4: Plot the outputs (final states) of this Neural ODE, as well as the analytic solutions, for a new set of inputs given below.**\n",
        "---\n",
        "```x = np.linspace(0,30,1000)```"
      ],
      "metadata": {
        "id": "zKXTk5UYUPvY"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}